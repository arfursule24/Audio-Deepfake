
# Papers Related to My Research:
|Reading Date | Title | Description | Tags | Month | Code | Dataset | Limitations | Papaer Link |
|--- |-------|-------------|------|-------|------|---------| ------| ---- |
| 23 Feb 2024 |ASVspoof 5 Challenge: Advanced ResNet Architectures for Robust Voice Spoofing Detection|In this paper, they have perfromed 2 experiments. 1. closed condition(Trained on ASVSpoof, tested on ASV Sppof 2024 and In the Wild data. 2. Open condition. Trained on different data set and tested on the same. with addition of each dataset in the cases (3 diffreent case were assumed) | audio deepfake detection using resnet 101 (Google Scholar Since 2024)|Aug 2024|No, Model description given| ASVspoof 2024. ASVSpoof 2019, ASVSpoof 2021, FoR,in The wild | Will need to focuss on robustness and generalization | https://www.isca-archive.org/asvspoof_2024/dao24_asvspoof.pdf |
| 24 Feb 2024 |Audio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion Attentive Classifier | In this paper, they have discussed about the WAVLM model as front end feature extractor and MAF as a classifier. All the layers and time is consider for the audio as an input. WIth different embeddings as compared with wav2vec2, wavLM is giving better results. Comparing the model with different ablations : Got the EER of 2.56 on Evaluation set of DF with WAVLM ft Large model. Refer to the paper for the results.| (Google Scholar Since 2024) | Jan 2024| Not available  |ASV Spoof 2019 LA & ASV Spoof 2021 LA & DF | Trained on 2019 training data and not on 2021 train data, pattern learnings are diffrent. All the layers are consider, to minize parametrs and no. of layers to give more accurate results, eliminating these layer can help us get good achivement. | https://arxiv.org/pdf/2312.08089 | 
| 25 Feb 2024 | Experimental analysis of features for replay attack detection–Results on the ASVspoof 2017 Challenge | In this paper, only replay attach was studied. Total 9 diffrenet features are consider (All coefficients mostly) and classified using 2-class Gaussian Mixture Models (GMM). There are different 2 experiments they carried out. Experiment 1, focusses on only ASV2017 data set (a. closed condition. b. Flexible condition [trained on ASV2017 data only but used external data for parameter selection]), and experiment 2 consist of cross databasei.e BTAS 2016 dataset. In this paper the have calculated EER individually and with 2 fusion systems too, like fusion 1 consists of cqcc+IMFCC+SCMC+Phrase ID. Whereas Fusion 2 consists of  : CQCC+LPSS+RFCC+Phrase ID.. The best individual result was for IMFCCs with dev EER of 3.85 but Evaluation eer of 30.91% (In closed condistion) where as in flexible condition got individual EER of 8.35 and with fusion got evaluation EER of 10.52 (fature used - RFCC+LFCC) | (Google Scholar Since 2024) | Jan 2024| Not available |ASV Spoof 2019 LA & ASV Spoof 2021 LA & DF | Only spectral features were compared | [https://arxiv.org/pdf/2312.08089] (https://www.isca-archive.org/interspeech_2017/font17_interspeech.pdf) |
| 26 Feb 2024 | A Comparison of Features for Synthetic Speech Detection | There is comparision of 19 handcrafted spectral features only of magnitude and phase for both short and long term features  is done. They have used GMM model and SVM model  | (Google Scholar Since 2024) | Jan 2024| Not available |ASV Spoof 2015 Corpus | Only spectral features are compared  | https://arxiv.org/pdf/2312.08089 https://erepo.uef.fi/server/api/core/bitstreams/b9ad5408-af55-4de1-a8fa-ab67931a2a29/content |
| 20 March 2025 | Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge | This paper was made because they wanted to submit it to the ASVSpoof 5 competition. This paper was based on trial and error run. It made various experiments, including data expansion, data augmentation and self-supervised learning features. So for data expansion, they have consider various data set, out of which they have used the best data they got the best results with similarly with data augmentation, and self-supervised. They have used pre-trained  self-supervised learning feature, like WavLM, wave2vec2 large and unispeech. Also audio of various length was consider and the best results they got were for 10 sec and then later on they have combined all the 7 best methodologies and then found the best result and submitted that to the competion. Though fr competition, they used an Evaluation test (progress set) and not the full evaluation set, the best eer and min dcf got was 0.00158 and 0.55% EER respectively | ISCA-archive | Aug 2024| Not available |ASV Spoof 2024 + MLAAD + Codecfake + ASVspoof 2019 | No fixed strategy, Did not perform well on Evaluation full set. | https://www.isca-archive.org/asvspoof_2024/xie24_asvspoof.pdf|
| 27 Feb 2024 | KAN: Kolmogorov–Arnold Networks | | (Google Scholar Since 2024) | Jan 2024| |ASV Spoof 2019 LA & ASV Spoof 2021 LA & DF |  | https://arxiv.org/pdf/2312.08089 |
| 08 March 2025 | Deepfake Audio Detection via MFCC Features Using Machine Learning | | (Google Scholar Since 2024) | Jan 2024| |ASV Spoof 2019 LA & ASV Spoof 2021 LA & DF |  | https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996362 |
| 08 March 2025 | Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection | | (Google Scholar Since 2024) | Jan 2024| |ASV Spoof 2019 LA & ASV Spoof 2021 LA & DF |  | https://www.isca-archive.org/interspeech_2023/xie23c_interspeech.pdf |
| 08 March 2025 | Audio Deepfake Detection With Self-Supervised Wavlm And Multi-Fusion Attentive Classifier | WavLM was used to extract feature and along with that multi-fusion attentive classifier based on attentive statistical pooling layer was also used. (MFA capture the complimentary info about the audio at both time and layer level) This experiment gave the state of the art method for ASVSpoof 2021 LA and 2019 set. The best result was with WavLM -L(ft) +MFC EER = 2.56% | ICASSP 2024 | Jan 2024| |ASV Spoof 2019 LA & ASV Spoof 2021 LA & DF | No Generalization good results found | https://www.isca-archive.org/interspeech_2023/xie23c_interspeech.pdf |
| 2 June 2025 | Wave-Spectrpgram Cross-Modal Aggregation for Audio DeepFake Detection | Combined Waveform (Raw audio) and Spectrogram of same audio. Waveform using Wav2vec2 and Spectrogram from Unet Multiscale spatial feature.  These two features were combined using CMFF (Cross Modal Feature Fusion) Dataset used : ASVSpoof 2021 and In the Wild dataset.  Results outperformed the SOTA methods. Did Ablation study. For loss they have combined both vross entropy loss and Single center loss (deals with forgery types). Used EER only. In this ablation study they have comparedspectrogram, waveform, Multiscale, CMFF and SCL (SCL is nothing but a loss)| ICASSP Journal | April 2025 | No. But Easy Model Used | Worked on 2 dataset ASVSpoof 2021 and In the Wild dataset | Limitations | https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10890563 |
